# 配置ssh是前提： 可以通过ssh_init包进行配置
# help的命令：

# hdfs dfs -help  ：
* hdfs dfs -help mv
-mv <src> ... <dst> :
  Move files that match the specified file pattern <src> to a destination <dst>.
When moving multiple files, the destination must be a directory.

* hdfs dfs -ls

* hdfs dfs -mkdir

* hdfs dfs -moveFromLocal ./local.txt /user/remote/test/

* hdfs dfs -appendToFile ./local.txt /user/remote/test/1.txt

* hdfs dfs -cat /user/remote/test/1.txt

* hdfs dfs -copyFromLocal ./local.txt /user/remote/test/

* hdfs dfs -coptToLocal /user/remote/test/1.txt ./

* hdfs dfs -cp /user/dir1/test/1.txt /user/dir2/test/

* hdfs dfs -get 同copyToLocal

* hdfs dfs -getmerge /user/test/*.txt ./jiu.txt 合并之后下载

* hdfs dfs -put 同copyFromLocal

* hdfs dfs -tail

* hdfs dfs -rm

* hdfs dfs -rmdir

* hdfs dfs -du

* hdfs dfs -setrep 10 /user/text.txt 设置副本数

* 小文件归档：
（2）归档文件
把/user/linyouyi/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/linyouyi/output路径下。

[linyouyi@hadoop01 hadoop-2.7.7]$ bin/hadoop archive -archiveName input.har –p /user/linyouyi/input /user/linyouyi/output
（3）查看归档

[linyouyi@hadoop01 hadoop-2.7.7]$ hadoop fs -lsr /user/linyouyi/output/input.har
[linyouyi@hadoop01 hadoop-2.7.7]$ hadoop fs -lsr har:///user/linyouyi/output/input.har
（4）解归档文件

[linyouyi@hadoop01 hadoop-2.7.7]$ hadoop fs -cp har:///user/linyouyi/output/input.har/* /user/linyouyi


